"""Use a discriminator to assess generative model performance."""
from typing import Iterator

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

DEFAULT_TEST_SIZE = 0.25
DEFAULT_RANDOM_STATE = 42
DEFAULT_N_ESTIMATORS = 100


def run_discriminator(
    true_data: np.ndarray,
    synthetic_data_samples: Iterator[np.ndarray],
    test_size: float = DEFAULT_TEST_SIZE,
    random_state: int = DEFAULT_RANDOM_STATE,
    n_estimators: int = DEFAULT_N_ESTIMATORS,
) -> pd.Series:
    """Run random forest discriminators between true data and every synthetic data sample file in a given directory.

    Args:
        true_data: Data from the true distribution.
        synthetic_data_samples: A collection of synthetic data samples, assumed to be presented in order.
        test_size: Fraction of the data to use as testing data.
        random_state: Seed for various `sklearn` randomized algorithms.
        n_estimators: Number of trees in our random forest.

    Returns:
        A pandas `Series` of the discriminator error.

    Example:
        >>> size = (100, 5)
        >>> run_discriminator(
        ...   np.zeros(size),
        ...   [np.ones(size), np.random.default_rng(1729).binomial(1, 0.17, size), np.zeros(size)]
        ... )
        0    0.00
        1    0.14
        2    0.50
        dtype: float64
    """
    return pd.Series(
        [
            discriminate(true_data, synthetic_data, test_size, random_state, n_estimators)
            for synthetic_data in synthetic_data_samples
        ]
    )


def discriminate(
    true_data: np.ndarray,
    synthetic_data: np.ndarray,
    test_size: float = DEFAULT_TEST_SIZE,
    random_state: int = DEFAULT_RANDOM_STATE,
    n_estimators: int = DEFAULT_N_ESTIMATORS,
) -> float:
    """Use a random forest to discriminate between true and synthetic data.

    Args:
        true_data: Data from the true distribution.
        synthetic_data: Data generated by our process to try and mimic the true data.
        test_size: Fraction of the data to use as testing data.
        random_state: Seed for various `sklearn` randomized algorithms.
        n_estimators: Number of trees in our random forest.

    Returns:
        Error (1 - accuracy) of the discriminating random forest.

    Example:
        >>> size = (100, 5)
        >>> discriminate(np.zeros(size), np.ones(size))
        0.0
        >>> discriminate(np.zeros(size), np.random.default_rng(1729).binomial(1, 0.17, size))
        0.14
        >>> discriminate(np.zeros(size), np.zeros(size))
        0.5
    """
    X = np.concatenate([true_data, synthetic_data])
    y = np.concatenate([np.zeros(len(true_data), dtype=int), np.ones(len(synthetic_data), dtype=int)])
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state, stratify=y
    )
    rf = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state, n_jobs=-1)
    rf.fit(X_train, y_train)
    return 1 - accuracy_score(y_test, rf.predict(X_test))
